{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.6.8 64-bit ('python': venv)",
   "display_name": "Python 3.6.8 64-bit ('python': venv)",
   "metadata": {
    "interpreter": {
     "hash": "c5a9beb1920113f7b0dd4fa992e2e3b2c22254a9a289d526365f43874d158a54"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for this notebook from here: https://www.kaggle.com/korfanakis/titanic-a-beginner-friendly-approach-to-top-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('precision', 3)\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style('dark')\n",
    "\n",
    "mpl.rcParams['axes.labelsize'] = 14\n",
    "mpl.rcParams['axes.titlesize'] = 15\n",
    "mpl.rcParams['xtick.labelsize'] = 12\n",
    "mpl.rcParams['ytick.labelsize'] = 12\n",
    "mpl.rcParams['legend.fontsize'] = 12\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler \n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict, GridSearchCV\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, VotingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\n",
    "\n",
    "print('Libraries Loaded!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('.\\\\Titanic\\\\Data\\\\Raw\\\\train.csv')\n",
    "test_df = pd.read_csv('.\\\\Titanic\\\\Data\\\\Raw\\\\test.csv')\n",
    "\n",
    "print('Dataframes loaded!')\n",
    "print('Training set: {} rows and {} columns'.format(train_df.shape[0], train_df.shape[1]))\n",
    "print('    Test set: {} rows and {} columns'.format(test_df.shape[0], test_df.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.concat([train_df, test_df])\n",
    "\n",
    "print('Combined set: {} rows and {} columns'.format(all_data.shape[0], all_data.shape[1]))\n",
    "print('\\nSurvived?: ')\n",
    "all_data['Survived'].value_counts(dropna = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.drop('PassengerId', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_counts = train_df.isnull().sum().sort_values(ascending = False)\n",
    "percent = (train_df.isnull().sum()*100/train_df.shape[0]).sort_values(ascending = False)\n",
    "\n",
    "missing_df = pd.concat([missing_counts, percent], axis = 1, keys = ['Counts', '%'])\n",
    "print('Missing values: ')\n",
    "missing_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_atts = ['Age', 'SibSp', 'Parch', 'Fare', 'Pclass']\n",
    "train_df[num_atts].hist(figsize = (15, 6), color = 'steelblue', edgecolor = 'firebrick', linewidth = 1.5, layout = (2, 3));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (11, 4))\n",
    "\n",
    "sns.countplot(x = 'Sex', hue = 'Survived', data = train_df,  palette = 'tab20', ax = ax1) \n",
    "ax1.set_title('Count of (non-)Survivors by Gender')\n",
    "ax1.set_xlabel('Gender')\n",
    "ax1.set_ylabel('Number of Passenger')\n",
    "ax1.legend(labels = ['Deceased', 'Survived'])\n",
    "\n",
    "sns.barplot(x = 'Sex', y = 'Survived', data = train_df,  palette = ['#94BFA7', '#FFC49B'], ci = None, ax = ax2)\n",
    "ax2.set_title('Survival Rate by Gender')\n",
    "ax2.set_xlabel('Gender')\n",
    "ax2.set_ylabel('Survival Rate');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "men = train_df[train_df['Sex']  == 'male']\n",
    "women = train_df[train_df['Sex']  == 'female']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize = (13, 4))\n",
    "\n",
    "sns.distplot(train_df[train_df['Survived'] == 1]['Age'].dropna(), bins = 20, label = 'Survived', ax = ax1, kde = False)\n",
    "sns.distplot(train_df[train_df['Survived'] == 0]['Age'].dropna(), bins = 20, label = 'Deceased', ax = ax1, kde = False)\n",
    "ax1.legend()\n",
    "ax1.set_title('Age Distribution - All Passengers')\n",
    "\n",
    "sns.distplot(women[women['Survived'] == 1]['Age'].dropna(), bins = 20, label = 'Survived', ax = ax2, kde = False)\n",
    "sns.distplot(women[women['Survived'] == 0]['Age'].dropna(), bins = 20, label = 'Deceased', ax = ax2, kde = False)\n",
    "ax2.legend()\n",
    "ax2.set_title('Age Distribution - Women')\n",
    "\n",
    "sns.distplot(men[men['Survived'] == 1]['Age'].dropna(), bins = 20, label = 'Survived', ax = ax3, kde = False)\n",
    "sns.distplot(men[men['Survived'] == 0]['Age'].dropna(), bins = 20, label = 'Deceased', ax = ax3, kde = False)\n",
    "ax3.legend()\n",
    "ax3.set_title('Age Distribution - Men')\n",
    "\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df['Age_Bin'] = pd.qcut(train_df['Age'], 4)  # Quantile-based discretization\n",
    "train_df['Age_Bin'] = (train_df['Age']//15)*15\n",
    "train_df[['Age_Bin', 'Survived']].groupby(['Age_Bin']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x = 'Embarked', hue = 'Survived', data = train_df,  palette = 'tab20') \n",
    "plt.ylabel('Number of Passenger')\n",
    "plt.title('Count of (non-)Survivors by Port of Embarkation')\n",
    "plt.legend(['Deceased', 'Survived']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of passengers in each class:')\n",
    "train_df['Pclass'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (12, 5))\n",
    "\n",
    "sns.countplot(x = 'Pclass', hue = 'Survived', data = train_df,  palette = 'tab20', ax = ax1) \n",
    "ax1.legend(['Deceased', 'Survived'])\n",
    "ax1.set_title('Count of (non-)Survivors by Class')\n",
    "ax1.set_ylabel('Number of Passengers')\n",
    "\n",
    "sns.barplot(x = 'Pclass', y = 'Survived', data = train_df,  palette = ['#C98BB9', '#F7D4BC', '#B5E2FA'], ci = None, ax = ax2)\n",
    "ax2.set_title('Survival Rate by Class')\n",
    "ax2.set_ylabel('Survival Rate');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (12, 5))\n",
    "\n",
    "sns.boxplot(x = 'Pclass', y = 'Fare', data = train_df, palette = 'tab20', ax = ax1)\n",
    "ax1.set_title('Distribution of Fares by Class')\n",
    "\n",
    "sns.distplot(train_df[train_df['Survived'] == 1]['Fare'], label = 'Survived', ax = ax2)\n",
    "sns.distplot(train_df[train_df['Survived'] == 0]['Fare'], label = 'Not Survived', ax = ax2)\n",
    "ax2.set_title('Distribution of Fares for (non-)Survivors')\n",
    "ax2.set_xlim([-20, 200])\n",
    "ax2.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['Fare_Bin'] = pd.qcut(train_df['Fare'], 5)\n",
    "train_df[['Fare_Bin', 'Survived']].groupby(['Fare_Bin']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alone = train_df[(train_df['SibSp'] == 0) & (train_df['Parch'] == 0)]\n",
    "not_alone = train_df[(train_df['SibSp'] != 0) | (train_df['Parch'] != 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (12, 5))\n",
    "\n",
    "sns.countplot(x = 'Survived', data = alone,  palette = 'tab20', ax = ax1) \n",
    "ax1.set_title('Count of Alone (non-)Survivors')\n",
    "ax1.set_xlabel('')\n",
    "ax1.set_xticklabels(['Deceased', 'Survived'])\n",
    "ax1.set_ylabel('Number of Passengers')\n",
    "\n",
    "sns.countplot(x = 'Survived', data = not_alone,  palette = 'tab20', ax = ax2) \n",
    "ax2.set_title('Count of (non-)Survivors with Family Onboard')\n",
    "ax2.set_xlabel('')\n",
    "ax2.set_xticklabels(['Deceased', 'Survived'])\n",
    "ax2.set_ylabel('Number of Passengers')\n",
    "\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['Relatives'] = train_df['SibSp'] + train_df['Parch']\n",
    "# train_df[['Relatives', 'Survived']].groupby(['Relatives']).mean()\n",
    "\n",
    "sns.factorplot('Relatives', 'Survived', data = train_df, color = 'firebrick', aspect = 1.5)\n",
    "plt.title('Survival rate by Number of Relatives Onboard');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['Title'] = train_df['Name'].apply(lambda x: x.split(',')[1].split('.')[0].strip())\n",
    "\n",
    "train_df['Title'].replace({'Mlle': 'Miss', 'Mme': 'Mrs', 'Ms': 'Miss'}, inplace = True)\n",
    "train_df['Title'].replace(['Don', 'Rev', 'Dr', 'Major', 'Lady', 'Sir', 'Col', 'Capt', 'the Countess', 'Jonkheer'],\n",
    "                           'Rare Title', inplace = True)\n",
    "train_df['Title'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['#067BC2', '#84BCDA', '#ECC30B', '#F37748', '#D56062']\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (10, 4))\n",
    "\n",
    "sns.countplot(x = 'Title', data = train_df,  palette = cols, ax = ax1)\n",
    "ax1.set_title('Passenger Count by Title')\n",
    "ax1.set_ylabel('Number of Passengers')\n",
    "\n",
    "sns.barplot(x = 'Title', y = 'Survived', data = train_df,  palette = cols, ci = None, ax = ax2)\n",
    "ax2.set_title('Survival Rate by Title')\n",
    "ax2.set_ylabel('Survival Rate');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Cabin:\\n  Number of existing values: ', train_df['Cabin'].notnull().sum())\n",
    "print ('    Number of unique values: ', train_df['Cabin'].nunique())"
   ]
  },
  {
   "source": [
    "| Attribute | Important | Action |\n",
    "| --- | --- | --- |\n",
    "| PassengerId | No | Discard |\n",
    "| Sex | Yes | Encode |\n",
    "| Age | Yes | Bin and Encode |\n",
    "| Port of Embarkation | No | Discard |\n",
    "| Pclass | Yes | N/A |\n",
    "| Fare | Yes | Bin and Encode |\n",
    "| SibSp and Parch | Yes | Engineer `Relatives` |\n",
    "| Name | Yes | Engineer `Title` and Encode |\n",
    "| Cabin | No | Discard |\n",
    "| Ticket | Yes | Engineer `Family_Survival` |"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['Age'] = all_data['Age'].fillna(train_df['Age'].median())\n",
    "all_data['Fare'] = all_data['Fare'].fillna(train_df['Fare'].median())\n",
    "print ('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again, the code for 'Family_Survival' comes from this kernel:\n",
    "# https://www.kaggle.com/konstantinmasich/titanic-0-82-0-83/notebook\n",
    "\n",
    "all_data['Last_Name'] = all_data['Name'].apply(lambda x: str.split(x, ',')[0])\n",
    "all_data['Fare'].fillna(all_data['Fare'].mean(), inplace = True)\n",
    "\n",
    "default_sr_value = 0.5\n",
    "all_data['Family_Survival'] = default_sr_value\n",
    "\n",
    "for grp, grp_df in all_data[['Survived','Name', 'Last_Name', 'Fare', 'Ticket', 'PassengerId', 'SibSp', 'Parch', 'Age', 'Cabin']].groupby(['Last_Name', 'Fare']):\n",
    "    \n",
    "    if (len(grp_df) != 1):  # A Family group is found.\n",
    "        for ind, row in grp_df.iterrows():\n",
    "            smax = grp_df.drop(ind)['Survived'].max()\n",
    "            smin = grp_df.drop(ind)['Survived'].min()\n",
    "            passID = row['PassengerId']\n",
    "            \n",
    "            if (smax == 1.0):\n",
    "                all_data.loc[all_data['PassengerId'] == passID, 'Family_Survival'] = 1\n",
    "            elif (smin==0.0):\n",
    "                all_data.loc[all_data['PassengerId'] == passID, 'Family_Survival'] = 0\n",
    "\n",
    "for _, grp_df in all_data.groupby('Ticket'):\n",
    "    \n",
    "    if (len(grp_df) != 1):\n",
    "        for ind, row in grp_df.iterrows():\n",
    "            if (row['Family_Survival'] == 0) | (row['Family_Survival']== 0.5):\n",
    "                smax = grp_df.drop(ind)['Survived'].max()\n",
    "                smin = grp_df.drop(ind)['Survived'].min()\n",
    "                passID = row['PassengerId']\n",
    "                \n",
    "                if (smax == 1.0):\n",
    "                    all_data.loc[all_data['PassengerId'] == passID, 'Family_Survival'] = 1\n",
    "                elif (smin==0.0):\n",
    "                    all_data.loc[all_data['PassengerId'] == passID, 'Family_Survival'] = 0\n",
    "                    \n",
    "#####################################################################################\n",
    "all_data['Age_Bin'] = (all_data['Age']//15)*15\n",
    "all_data['Fare_Bin'] = pd.qcut(all_data['Fare'], 5)\n",
    "all_data['Relatives'] = all_data['SibSp'] + all_data['Parch']\n",
    "#####################################################################################\n",
    "all_data['Title'] = all_data['Name'].apply(lambda x: x.split(',')[1].split('.')[0].strip())\n",
    "all_data['Title'].replace({'Mlle':'Miss', 'Mme':'Mrs', 'Ms':'Miss'}, inplace = True)\n",
    "all_data['Title'].replace(['Don', 'Rev', 'Dr', 'Major', 'Lady', 'Sir', 'Col', 'Capt', 'the Countess', 'Jonkheer', 'Dona'],\n",
    "                           'Rare Title', inplace = True)    \n",
    "\n",
    "print ('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['Fare_Bin'] = LabelEncoder().fit_transform(all_data['Fare_Bin'])\n",
    "all_data['Age_Bin'] = LabelEncoder().fit_transform(all_data['Age_Bin'])\n",
    "all_data['Title_Bin'] = LabelEncoder().fit_transform(all_data['Title'])\n",
    "all_data['Sex'] = LabelEncoder().fit_transform(all_data['Sex'])\n",
    "\n",
    "print ('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.drop(['PassengerId', 'Age', 'Fare', 'Name', 'SibSp', 'Parch', 'Ticket', 'Cabin', 'Title', 'Last_Name', 'Embarked'], axis = 1, inplace = True)\n",
    "\n",
    "print ('Done!')\n",
    "print ('Modified dataset: ')\n",
    "all_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = all_data[:891]\n",
    "\n",
    "X_train = train_df.drop('Survived', 1)\n",
    "y_train = train_df['Survived']\n",
    "\n",
    "#######################################################\n",
    "\n",
    "test_df = all_data[891:]\n",
    "\n",
    "X_test = test_df.copy()\n",
    "X_test.drop('Survived', axis = 1, inplace = True)\n",
    "print ('Splitting: Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_scaler = StandardScaler()\n",
    "\n",
    "X_train_scaled = std_scaler.fit_transform(X_train)  # fit_transform the X_train\n",
    "X_test_scaled = std_scaler.transform(X_test)        # only transform the X_test\n",
    "\n",
    "print ('Scaling: Done!')"
   ]
  },
  {
   "source": [
    "[Classifiers](https://www.oreilly.com/library/view/hands-on-machine-learning/9781491962282/)\n",
    "\n",
    "- Gaussian Naive Bayes\n",
    "- Logistic Regression\n",
    "- K-Nearest Neighbor Classifier\n",
    "- Support Vector Classifier\n",
    "- Decision Tree Classifier\n",
    "- Random Forest Classifier\n",
    "- XGBoosting Classifier\n",
    "- AdaBoost classifier"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 1021\n",
    "\n",
    "# Step 1: create a list containing all estimators with their default parameters\n",
    "clf_list = [\n",
    "    GaussianNB(), \n",
    "    LogisticRegression(random_state = random_state),\n",
    "    KNeighborsClassifier(), \n",
    "    SVC(random_state = random_state, probability = True),\n",
    "    DecisionTreeClassifier(random_state = random_state), \n",
    "    RandomForestClassifier(random_state = random_state),\n",
    "    XGBClassifier(random_state = random_state), \n",
    "    AdaBoostClassifier(base_estimator = DecisionTreeClassifier(random_state = random_state), random_state = random_state)\n",
    "]\n",
    "\n",
    "\n",
    "# Step 2: calculate the cv mean and standard deviation for each one of them\n",
    "cv_base_mean, cv_std = [], []\n",
    "for clf in clf_list:  \n",
    "    \n",
    "    cv = cross_val_score(clf, X_train_scaled, y = y_train, scoring = 'accuracy', cv = 5, n_jobs = -1)\n",
    "    \n",
    "    cv_base_mean.append(cv.mean())\n",
    "    cv_std.append(cv.std())\n",
    "\n",
    "    \n",
    "# Step 3: create a dataframe and plot the mean with error bars\n",
    "cv_total = pd.DataFrame({'Algorithm': ['Gaussian Naive Bayes', 'Logistic Regression', 'k-Nearest Neighboors', 'SVC', 'Decision Tree', 'Random Forest', 'XGB Classifier', 'AdaBoost Classifier'],\n",
    "                         'CV-Means': cv_base_mean, \n",
    "                         'CV-Errors': cv_std})\n",
    "\n",
    "sns.barplot('CV-Means', 'Algorithm', data = cv_total, palette = 'Paired', orient = 'h', **{'xerr': cv_std})\n",
    "plt.xlabel('Mean Accuracy')\n",
    "plt.title('Cross Validation Scores')\n",
    "plt.xlim([0.725, 0.88])\n",
    "plt.axvline(x = 0.80, color = 'firebrick', linestyle = '--');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = [('gnb', clf_list[0]), ('lr', clf_list[1]),\n",
    "              ('knn', clf_list[2]), ('svc', clf_list[3]),\n",
    "              ('dt', clf_list[4]), ('rf', clf_list[5]),\n",
    "              ('xgb', clf_list[6]), ('ada', clf_list[7])]\n",
    "\n",
    "base_voting_hard = VotingClassifier(estimators = estimators , voting = 'hard')\n",
    "base_voting_soft = VotingClassifier(estimators = estimators , voting = 'soft') \n",
    "\n",
    "cv_hard = cross_val_score(base_voting_hard, X_train_scaled, y_train, cv = 5)\n",
    "cv_soft = cross_val_score(base_voting_soft, X_train_scaled, y_train, cv = 5)\n",
    "\n",
    "print ('Baseline Models - Ensemble\\n--------------------------')\n",
    "print ('Hard Voting: {}%'.format(np.round(cv_hard.mean()*100, 1)))\n",
    "print ('Soft Voting: {}%'.format(np.round(cv_soft.mean()*100, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_voting_hard.fit(X_train_scaled, y_train)\n",
    "base_voting_soft.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_pred_base_hard = base_voting_hard.predict(X_test_scaled)\n",
    "y_pred_base_soft = base_voting_hard.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_means_tuned = [np.nan] # we can't actually tune the GNB classifier, so we fill its element with NaN\n",
    "\n",
    "#simple performance reporting function\n",
    "def clf_performance(classifier, model_name):\n",
    "    print(model_name)\n",
    "    print('-------------------------------')\n",
    "    print('   Best Score: ' + str(classifier.best_score_))\n",
    "    print('   Best Parameters: ' + str(classifier.best_params_))\n",
    "    \n",
    "    cv_means_tuned.append(classifier.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "\n",
    "param_grid = {'max_iter' : [100],\n",
    "              'penalty' : ['l1', 'l2'],\n",
    "              'C' : np.logspace(-2, 2, 20),\n",
    "              'solver' : ['lbfgs', 'liblinear']}\n",
    "\n",
    "clf_lr = GridSearchCV(lr, param_grid = param_grid, cv = 5, verbose = True, n_jobs = -1)\n",
    "\n",
    "best_clf_lr = clf_lr.fit(X_train_scaled, y_train)\n",
    "clf_performance(best_clf_lr, 'Logistic Regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_neighbors = np.concatenate((np.arange(3, 30, 1), np.arange(22, 32, 2)))\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "param_grid = {'n_neighbors' : np.arange(3, 30, 2),\n",
    "              'weights': ['uniform', 'distance'],\n",
    "              'algorithm': ['auto'],\n",
    "              'p': [1, 2]}\n",
    "\n",
    "clf_knn = GridSearchCV(knn, param_grid = param_grid, cv = 5, verbose = True, n_jobs = -1)\n",
    "best_clf_knn = clf_knn.fit(X_train_scaled, y_train)\n",
    "clf_performance(best_clf_knn, 'KNN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC(probability = True)\n",
    "param_grid = tuned_parameters = [{'kernel': ['rbf'], \n",
    "                                  'gamma': [0.01, 0.1, 0.5, 1, 2, 5],\n",
    "                                  'C': [.1, 1, 2, 5]},\n",
    "                                 {'kernel': ['linear'], \n",
    "                                  'C': [.1, 1, 2, 10]},\n",
    "                                 {'kernel': ['poly'], \n",
    "                                  'degree' : [2, 3, 4, 5], \n",
    "                                  'C': [.1, 1, 10]}]\n",
    "\n",
    "clf_svc = GridSearchCV(svc, param_grid = param_grid, cv = 5, verbose = True, n_jobs = -1)\n",
    "best_clf_svc = clf_svc.fit(X_train_scaled, y_train)\n",
    "clf_performance(best_clf_svc, 'SVC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(random_state = 1)\n",
    "param_grid = {'max_depth': [3, 5, 10, 20, 50],\n",
    "              'criterion': ['entropy', 'gini'],\n",
    "              'min_samples_split': [5, 10, 15, 30],\n",
    "              'max_features': [None, 'auto', 'sqrt', 'log2']}\n",
    "                                  \n",
    "clf_dt = GridSearchCV(dt, param_grid = param_grid, cv = 5, verbose = True, n_jobs = -1)\n",
    "best_clf_dt = clf_dt.fit(X_train_scaled, y_train)\n",
    "clf_performance(best_clf_dt, 'Decision Tree')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(random_state = 42)\n",
    "param_grid = {'n_estimators': [50, 150, 300, 450],\n",
    "              'criterion': ['entropy'],\n",
    "              'bootstrap': [True],\n",
    "              'max_depth': [3, 5, 10],\n",
    "              'max_features': ['auto','sqrt'],\n",
    "              'min_samples_leaf': [2, 3],\n",
    "              'min_samples_split': [2, 3]}\n",
    "                                  \n",
    "clf_rf = GridSearchCV(rf, param_grid = param_grid, cv = 5, verbose = True, n_jobs = -1)\n",
    "best_clf_rf = clf_rf.fit(X_train_scaled, y_train)\n",
    "clf_performance(best_clf_rf, 'Random Forest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_rf = best_clf_rf.best_estimator_\n",
    "\n",
    "importances = pd.DataFrame({'Feature': X_train.columns,\n",
    "                            'Importance': np.round(best_rf.feature_importances_, 3)})\n",
    "\n",
    "importances = importances.sort_values('Importance', ascending = True).set_index('Feature')\n",
    "\n",
    "importances.plot.barh(color = 'steelblue', edgecolor = 'firebrick', legend=False)\n",
    "plt.title('Random Forest Classifier')\n",
    "plt.xlabel('Importance');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBClassifier(random_state = 42)\n",
    "\n",
    "param_grid = {'n_estimators': [15, 25, 50, 100],\n",
    "              'colsample_bytree': [0.65, 0.75, 0.80],\n",
    "              'max_depth': [None],\n",
    "              'reg_alpha': [1],\n",
    "              'reg_lambda': [1, 2, 5],\n",
    "              'subsample': [0.50, 0.75, 1.00],\n",
    "              'learning_rate': [0.01, 0.1, 0.5],\n",
    "              'gamma': [0.5, 1, 2, 5],\n",
    "              'min_child_weight': [0.01],\n",
    "              'sampling_method': ['uniform']}\n",
    "\n",
    "clf_xgb = GridSearchCV(xgb, param_grid = param_grid, cv = 3, verbose = True, n_jobs = -1)\n",
    "best_clf_xgb = clf_xgb.fit(X_train_scaled, y_train)\n",
    "clf_performance(best_clf_xgb, 'XGB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_xgb = best_clf_xgb.best_estimator_\n",
    "\n",
    "importances = pd.DataFrame({'Feature': X_train.columns,\n",
    "                            'Importance': np.round(best_xgb.feature_importances_, 3)})\n",
    "\n",
    "importances = importances.sort_values('Importance', ascending = True).set_index('Feature')\n",
    "\n",
    "importances.plot.barh(color = 'darkgray', edgecolor = 'firebrick', legend = False)\n",
    "plt.title('XGBoost Classifier')\n",
    "plt.xlabel('Importance');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adaDTC = AdaBoostClassifier(base_estimator = DecisionTreeClassifier(random_state = random_state), random_state=random_state)\n",
    "\n",
    "param_grid = {'algorithm': ['SAMME', 'SAMME.R'],\n",
    "              'base_estimator__criterion' : ['gini', 'entropy'],\n",
    "              'base_estimator__splitter' : ['best', 'random'],\n",
    "              'n_estimators': [2, 5, 10, 50],\n",
    "              'learning_rate': [0.01, 0.1, 0.2, 0.3, 1, 2]}\n",
    "\n",
    "clf_ada = GridSearchCV(adaDTC, param_grid = param_grid, cv = 5, scoring = 'accuracy', n_jobs = -1, verbose = 1)\n",
    "best_clf_ada = clf_ada.fit(X_train_scaled, y_train)\n",
    "\n",
    "clf_performance(best_clf_ada, 'AdaBost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_ada = best_clf_ada.best_estimator_\n",
    "importances = pd.DataFrame({'Feature': X_train.columns,\n",
    "                            'Importance': np.round(best_ada.feature_importances_, 3)})\n",
    "\n",
    "importances = importances.sort_values('Importance', ascending = True).set_index('Feature')\n",
    "\n",
    "importances.plot.barh(color = 'cadetblue', edgecolor = 'firebrick', legend = False)\n",
    "plt.title('AdaBoost Classifier')\n",
    "plt.xlabel('Importance');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_total = pd.DataFrame({\n",
    "    'Algorithm': [\n",
    "        'Gaussian Naive Bayes', \n",
    "        'Logistic Regression', \n",
    "        'k-Nearest Neighboors', \n",
    "        'SVC', \n",
    "        'Decision Tree', \n",
    "        'Random Forest', \n",
    "        'XGB Classifier', \n",
    "        'AdaBoost Classifier'\n",
    "        ],\n",
    "    'Baseline': cv_base_mean, \n",
    "    'Tuned Performance': cv_means_tuned})\n",
    "\n",
    "cv_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_lr = best_clf_lr.best_estimator_\n",
    "best_knn = best_clf_knn.best_estimator_\n",
    "best_svc = best_clf_svc.best_estimator_\n",
    "best_dt = best_clf_dt.best_estimator_\n",
    "best_rf = best_clf_rf.best_estimator_\n",
    "best_xgb = best_clf_xgb.best_estimator_\n",
    "# best_ada = best_clf_ada.best_estimator_  # didn't help me in my final ensemble\n",
    "\n",
    "estimators = [('lr', best_lr), ('knn', best_knn), ('svc', best_svc),\n",
    "              ('rf', best_rf), ('xgb', best_xgb), ('dt', best_dt)]\n",
    "\n",
    "tuned_voting_hard = VotingClassifier(estimators = estimators, voting = 'hard', n_jobs = -1)\n",
    "tuned_voting_soft = VotingClassifier(estimators = estimators, voting = 'soft', n_jobs = -1)\n",
    "\n",
    "tuned_voting_hard.fit(X_train_scaled, y_train)\n",
    "tuned_voting_soft.fit(X_train_scaled, y_train)\n",
    "\n",
    "cv_hard = cross_val_score(tuned_voting_hard, X_train_scaled, y_train, cv = 5)\n",
    "cv_soft = cross_val_score(tuned_voting_soft, X_train_scaled, y_train, cv = 5)\n",
    "\n",
    "print ('Tuned Models - Ensemble\\n-----------------------')\n",
    "print ('Hard Voting: {}%'.format(np.round(cv_hard.mean()*100, 2)))\n",
    "print ('Soft Voting: {}%'.format(np.round(cv_soft.mean()*100, 2)))\n",
    "\n",
    "y_pred_tuned_hd = tuned_voting_hard.predict(X_test_scaled).astype(int)\n",
    "y_pred_tuned_sf = tuned_voting_soft.predict(X_test_scaled).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.DataFrame(pd.read_csv('.\\\\Titanic\\\\Data\\\\Raw\\\\test.csv')['PassengerId'])\n",
    "\n",
    "pd.DataFrame(data = {'PassengerId': test_df.PassengerId, \n",
    "                     'Survived': y_pred_base_hard.astype(int)}).to_csv(\n",
    "                         '.\\\\Titanic\\\\Artifacts\\\\01-Baseline_Hard_voting.csv', index = False)\n",
    "\n",
    "pd.DataFrame(data = {'PassengerId': test_df.PassengerId, \n",
    "                     'Survived': y_pred_base_soft.astype(int)}).to_csv(\n",
    "                         '.\\\\Titanic\\\\Artifacts\\\\02-Baseline_Soft_voting.csv', index = False)\n",
    "\n",
    "pd.DataFrame(data = {'PassengerId': test_df.PassengerId, \n",
    "                     'Survived': y_pred_tuned_hd.astype(int)}).to_csv(\n",
    "                         '.\\\\Titanic\\\\Artifacts\\\\03-Tuned_Hard_Voting.csv', index = False)\n",
    "\n",
    "pd.DataFrame(data = {'PassengerId': test_df.PassengerId, \n",
    "                     'Survived': y_pred_tuned_sf.astype(int)}).to_csv(\n",
    "                         '.\\\\Titanic\\\\Artifacts\\\\04-Tuned_Soft_Voting.csv', index = False)"
   ]
  }
 ]
}